# ğŸ® Project LANTERN  
### Multi-Agent OCR, Reconstruction & Insight Extraction System  
**Built for the Google AI Agents Intensive â€” 2025 Capstone**

Project LANTERN is a production-inspired, multi-agent pipeline that transforms noisy, unstructured document archives into clean, enriched, searchable intelligence.

Designed for **performance**, **traceability**, and **clarity**, the system uses modular agents to:

- ğŸ” Perform OCR (Gemini Vision â†’ OpenAI â†’ Local Mock fallback)
- ğŸ§µ Reconstruct multi-page threads from manifest metadata
- ğŸ§  Extract structured insights, summaries, entities, and metadata
- ğŸ—‚ Build a fast DuckDB search database  
- ğŸ“Š Present OCR + structured enrichment via interactive notebooks

This project demonstrates **real-world multi-agent system design**, **resilient fallback paths**, and a **polished, end-to-end data pipeline** fit for enterprise scenarios.

---

## ğŸ“ Architecture Overview

Below is the core project flow as a **Mermaid system diagram** (renders automatically on GitHub):

```mermaid
flowchart TD

    A[ğŸ“ Raw Scans<br>JPG/PNG] --> B[ğŸŸ¦ OCR Agent<br>(Gemini / OpenAI / Mock)]
    B --> C[ğŸ“ OCR Outputs<br>raw_text, clean_text, confidence]
    C --> D[ğŸ§µ Threading Agent<br>sequence grouping]

    D --> E[ğŸ§  Extraction Agent<br>page + sequence summaries<br>entities, search_text]
    E --> F[ğŸ“¤ JSONL Exports<br>pages.jsonl<br>sequences.jsonl]
    F --> G[ğŸ—‚ DuckDB Index<br>lantern.duckdb]

    G --> H[ğŸ” Search + Filtering<br>keyword + metadata]
    E --> I[ğŸ“Š Notebook Visualizations<br>image + OCR + insights side-by-side]

---

## ğŸ“¦ Project Structure

project-lantern/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ ocr_agent.py              # backend-agnostic OCR
â”‚   â”œâ”€â”€ threading_agent.py        # manifest-driven thread reconstruction
â”‚   â””â”€â”€ extraction_agent.py       # entities, summaries, enrichment
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ pipeline.py               # high-level orchestration
â”‚   â”œâ”€â”€ search_index.py           # DuckDB builder
â”‚   â””â”€â”€ utils_env.py              # environment loader
â”‚
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ cleanup_tools.py          # text normalization
â”‚   â”œâ”€â”€ ocr_tools.py              # OCR fallback utility
â”‚   â””â”€â”€ jsonl_tools.py            # read/write JSONL
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ defaults.env              # example config
â”‚   â”œâ”€â”€ feature_flags.yaml        # optional
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ manifest.csv
â”‚   â”œâ”€â”€ images/                   # input scans
â”‚   â””â”€â”€ outputs/
â”‚       â”œâ”€â”€ pages.jsonl
â”‚       â””â”€â”€ sequences.jsonl
â”‚
â””â”€â”€ notebooks/
    â”œâ”€â”€ 0_Setup_and_Environment.ipynb
    â”œâ”€â”€ 5_Visualization_Walkthrough.ipynb
    â”œâ”€â”€ project-lantern-architecture.png
    â””â”€â”€ ...

---

## ğŸš€ Key Features (Judge-Facing Summary)
ğŸ” 1. OCR Agent â€” Resilient, Backend-Agnostic

- Gemini Vision â†’ preferred
- OpenAI Vision â†’ fallback
- Local deterministic stub â†’ guaranteed execution
- Uniform output schema:

{
  "raw_text": "...",
  "clean_text": "...",
  "confidence": 0.91,
  "error": null
}

---

## ğŸ§µ 2. Thread Reconstruction Agent

- Uses manifest metadata (sequence_id, sequence_order)
- Gracefully handles missing fields
- Produces consistent sequence maps for extraction workflows
- Detects singletons automatically

---

## ğŸ§  3. Extraction Agent â€” Intelligent, Deterministic, Replaceable

- Page-level summaries
- Sequence-level summaries
- Lightweight entity extraction
- Search-ready search_text
- Deterministic mode (no LLM needed)
- Optional LLM-enhanced mode

---

## ğŸ—‚ 4. DuckDB Search Database

- Fast, portable, single-file DB
- Indexes:
* OCR text
* Entities
* Categories
* Document types
* Summaries

Perfect for demonstration and extension.

---

## ğŸ“Š 5. Notebook Visual Experience

- Side-by-side: scanned image + OCR + insights
- DuckDB search walkthrough
- Thread-level analysis

---

## ğŸ” Environment & Authentication

Project LANTERN uses a .env file to keep credentials safe and portable.

.env Template (Do NOT commit real keys)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Project LANTERN â€” Environment Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# --- OpenAI (optional fallback OCR) ---
OPENAI_API_KEY="YOUR_OPENAI_API_KEY_HERE"

# --- Google Gemini / Vertex AI ---
# Primary Vertex API key
GCP_API_KEY="YOUR_VERTEX_API_KEY_HERE"

# Optional Gemini Studio key
GOOGLE_API_KEY="YOUR_GEMINI_STUDIO_KEY_HERE"

# GCP Project Info
GCP_PROJECT_ID="gen-lang-client-0048713898"
GCP_LOCATION="us-central1"

# Service account JSON (for IAM auth only)
GOOGLE_APPLICATION_CREDENTIALS=""

# Models
GEMINI_OCR_MODEL="gemini-1.5-flash"
GEMINI_VISION_MODEL="gemini-1.5-pro"

# Feature Flags
USE_OCR_CACHE="true"
ENABLE_ENTITIES="true"
ENABLE_SEQUENCE_SUMMARY="true"
ENABLE_DEBUG_LOGS="false"

# Pipeline Settings
OCR_TIMEOUT_SECONDS="30"
MAX_OCR_RETRIES="2"
MAX_SUMMARY_CHARS="600"

# Paths
PROJECT_ROOT="./"
DATA_ROOT="./data"
OCR_CACHE_DIR="./data/ocr_cache"
EXPORT_DIR="./data/outputs"

---

## ğŸ”„ Authentication Rules (Local vs Kaggle vs GCP)

| Environment | Recommended Auth              | Key to Use                       | Notes                                     |
| ----------- | ----------------------------- | -------------------------------- | ----------------------------------------- |
| **Kaggle**  | Gemini Studio Key via Secrets | `GOOGLE_API_KEY`                 | Provided by the course                    |
| **Local**   | Vertex / Gemini API Key       | `GCP_API_KEY`                    | Loaded via `utils_env.load_environment()` |
| **GCP VM**  | Service Account JSON (IAM)    | `GOOGLE_APPLICATION_CREDENTIALS` | Auto-detected via ADC                     |

---

## âš™ï¸ Quickstart

1. Install dependencies
pip install -r requirements.txt

or minimal:
pip install duckdb python-dotenv pillow plotly pandas

---

2. ğŸ§ª Running the Pipeline (Core Flow)

from src.pipeline import run_pipeline
from src.utils_env import load_environment
from agents.ocr_agent import OCRAgent
from agents.threading_agent import ThreadingAgent
from agents.extraction_agent import ExtractionAgent

# 1) Load config from .env
cfg = load_environment()

# 2) Initialize agents
ocr = OCRAgent.from_env(cfg)
threader = ThreadingAgent()
extract = ExtractionAgent.from_env(cfg)

# 3) Run
pages, sequences = run_pipeline(
    manifest_df,
    ocr_agent=ocr,
    threading_agent=threader,
    extraction_agent=extract,
)

print(f"âœ… Pipeline complete. Pages: {len(pages)}, Sequences: {len(sequences)}")

---

3. ğŸ” Search with DuckDB

from src.search_index import open_duckdb

con = open_duckdb("data/lantern.duckdb")

results = con.sql("""
SELECT *
FROM pages
WHERE search_text LIKE '%court%'
ORDER BY sequence_id, sequence_order
LIMIT 20;
""").df()

results.head()

---

## ğŸ“Š Notebook Demonstration

Use notebooks/5_Visualization_Walkthrough.ipynb to:
- Display image + OCR + structured insights
- Walk through sequences
- Query DuckDB search results

This is the primary demo surface for judges.

---

## ğŸ… Why LANTERN Stands Out

âœ” Clean multi-agent architecture
âœ” Backend-agnostic OCR strategy
âœ” Manifest-driven sequencing
âœ” Deterministic + LLM-enhanced extraction
âœ” DuckDB-powered intelligence layer
âœ” High-quality, production-style documentation

---

## ğŸ‘¤ Author

**Payton K. Maurer**  
Senior Data Strategy & Digital Analytics Consultant  
Specializing in multi-agent systems, GA4/GTM architecture, and large-scale data pipelines.  

- ğŸ“§ payton.k.maurer@gmail.com  
- ğŸŒ LinkedIn: https://www.linkedin.com/in/paytonmaurer 

---

## ğŸ¤ Acknowledgements

Built for the Google AI Agents Intensive (2025).
Special thanks to the cohort leads, reviewers, and community.
