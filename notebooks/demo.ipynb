{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d24dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.ipynb\n",
    "\n",
    "# 1) Basic environment + path setup\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üîß Project LANTERN ‚Äî Demo Runner\")\n",
    "\n",
    "# Detect project root (assumes this notebook lives in <repo>/notebooks)\n",
    "notebook_cwd = Path.cwd().resolve()\n",
    "\n",
    "if notebook_cwd.name == \"notebooks\" and (notebook_cwd.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = notebook_cwd.parent\n",
    "elif (notebook_cwd / \"data\").exists() and (notebook_cwd / \"config\").exists():\n",
    "    PROJECT_ROOT = notebook_cwd\n",
    "else:\n",
    "    PROJECT_ROOT = notebook_cwd.parent\n",
    "\n",
    "DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "CONFIG_ROOT = PROJECT_ROOT / \"config\"\n",
    "EPSTEIN_IMAGE_ROOT = DATA_ROOT / \"epstein_curated_v1\"\n",
    "OCR_CACHE_DIR = DATA_ROOT / \"ocr_cache\"\n",
    "EXPORT_DIR = DATA_ROOT / \"outputs\"\n",
    "\n",
    "print(f\"üìÅ PROJECT_ROOT  : {PROJECT_ROOT}\")\n",
    "print(f\"üìÅ DATA_ROOT     : {DATA_ROOT}\")\n",
    "print(f\"üìÅ CONFIG_ROOT   : {CONFIG_ROOT}\")\n",
    "\n",
    "assert DATA_ROOT.exists(), \"DATA_ROOT missing (expected data/ at repo root).\"\n",
    "assert CONFIG_ROOT.exists(), \"CONFIG_ROOT missing (expected config/ at repo root).\"\n",
    "\n",
    "OCR_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Wire repo into sys.path\n",
    "for p in [PROJECT_ROOT, PROJECT_ROOT / \"src\", PROJECT_ROOT / \"agents\", PROJECT_ROOT / \"tools\"]:\n",
    "    p_str = str(p)\n",
    "    if p_str not in sys.path:\n",
    "        sys.path.insert(0, p_str)\n",
    "\n",
    "# 2) Imports from the project\n",
    "from agents.ocr_agent import OCRAgent, OCRAgentConfig\n",
    "from agents.threading_agent import ThreadingAgent, ThreadingAgentConfig\n",
    "from agents.extraction_agent import ExtractionAgent, ExtractionConfig\n",
    "from src.pipeline import run_pipeline\n",
    "\n",
    "print(\"‚úÖ Imports OK ‚Äî OCRAgent, ThreadingAgent, ExtractionAgent, run_pipeline\")\n",
    "\n",
    "# 3) Load full manifest\n",
    "manifest_path = DATA_ROOT / \"manifest.csv\"\n",
    "assert manifest_path.exists(), f\"Manifest not found at: {manifest_path}\"\n",
    "\n",
    "manifest_df = pd.read_csv(manifest_path)\n",
    "print(f\"‚úÖ Loaded manifest with {len(manifest_df)} rows.\")\n",
    "\n",
    "# 4) Instantiate agents with sensible defaults\n",
    "ocr_config = OCRAgentConfig(\n",
    "    model_name=\"gpt-4o-mini\",   # or \"gpt-4o\" in richer environments\n",
    "    max_retries=2,\n",
    "    timeout=30,\n",
    ")\n",
    "ocr_agent = OCRAgent(ocr_config)\n",
    "\n",
    "threading_agent = ThreadingAgent(enable_threads=True)\n",
    "\n",
    "extraction_config = ExtractionConfig(\n",
    "    enable_insights=True,\n",
    "    max_summary_chars=600,\n",
    "    debug=False,\n",
    ")\n",
    "extraction_agent = ExtractionAgent(extraction_config)\n",
    "\n",
    "print(\"‚úÖ Agents instantiated.\")\n",
    "\n",
    "# 5) Run the pipeline on the full manifest\n",
    "from time import perf_counter\n",
    "\n",
    "print(\"\\nüöÄ Running full Project LANTERN pipeline on manifest.csv\")\n",
    "print(f\"   OCR cache dir : {OCR_CACHE_DIR}\")\n",
    "print(f\"   Export dir    : {EXPORT_DIR}\")\n",
    "print(\"‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\")\n",
    "\n",
    "start = perf_counter()\n",
    "\n",
    "enriched_pages, sequence_summaries = run_pipeline(\n",
    "    manifest_df=manifest_df,\n",
    "    ocr_agent=ocr_agent,\n",
    "    threading_agent=threading_agent,\n",
    "    extraction_agent=extraction_agent,\n",
    "    epstein_image_root=EPSTEIN_IMAGE_ROOT,\n",
    "    ocr_cache_dir=OCR_CACHE_DIR,\n",
    "    use_ocr_cache=True,\n",
    "    save_ocr_cache=True,\n",
    "    export_dir=EXPORT_DIR,\n",
    "    export_jsonl=True,\n",
    ")\n",
    "\n",
    "elapsed = perf_counter() - start\n",
    "\n",
    "print(\"\\n‚úÖ Demo pipeline complete.\")\n",
    "print(f\"   ‚Ä¢ Enriched pages     : {len(enriched_pages)}\")\n",
    "print(f\"   ‚Ä¢ Sequence summaries : {len(sequence_summaries)}\")\n",
    "print(f\"   ‚Ä¢ Elapsed time       : {elapsed:0.2f} seconds\")\n",
    "print(\"\")\n",
    "print(\"Artifacts written to:\")\n",
    "print(f\"   ‚Ä¢ {EXPORT_DIR / 'pages.jsonl'}\")\n",
    "print(f\"   ‚Ä¢ {EXPORT_DIR / 'sequences.jsonl'}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
